# 刘度 - 大数据开发工程师个人简历
## 基本信息
|项目|详情|
| ---- | ---- |
|姓名|刘度|
|年龄|29岁|
|电话|13380365400（微信同号）|
|邮箱|liudu2326526@163.com|
|住址|广东省深圳市宝安区|
|工作经验|8年|
|工作状态|在职|
|入职时间|一个月内|
|毕业院校|四川大学|
|学历|本科|
|求职意向|大数据开发工程师|

## 工作经历
1. **2022.09 - 至今**：东信时代信息技术有限公司，数据组长（4人团队）
2. **2020.09 - 2022.08**：大宇无限科技有限公司，高级大数据开发工程师
3. **2017.06 - 2020.08**：济南铭函通信技术有限公司，大数据开发工程师

## 职业技能
### 数据建模
掌握 kimball 数仓维度建模理论，有数据治理及数据质量设计经验，具备离线、实时数仓生命周期建设经验。

### 数据分析
熟悉指标体系、监控体系、用户画像、A/B Test平台的搭建经验，精通用户生命周期指标模型AARRR模型，能通过回归、聚类等算法深入分析数据，提炼关键洞察并驱动业务决策。

### 数据治理
拥有数据质量、元数据管理、数据血缘系统建设经验，主导公司数据开发规范、数仓建设规范的编写与执行。

### 数据采集
具备Flink CDC、Flink+Kafka、Flume、Sqoop、Maxwell/Canal数据采集组件的开发经验。

### 数据计算
1. 熟练编写HQL、SparkSQL脚本，掌握UDF、UDAF、UDTF自定义函数开发，拥有丰富的SparkSQL调优经验，可解决各类慢SQL及数据倾斜问题。
2. 熟练使用Flink计算引擎，具备DataStream流式处理、窗口函数、定时器、FlinkSQL等开发经验，熟悉Exactly-once、CheckPoint机制，有Flink调优经验，能灵活运用Flink sql、算子、算子状态、窗口处理数据流。

### 数据存储
1. 熟悉Hadoop体系架构，有HDFS、Yarn、Mapreduce等组件使用经验，了解Yarn任务提交流程、MR任务运行流程等Hadoop底层原理，能结合业务场景选择数据异构存储、资源调度策略、存储/压缩格式。
2. 熟悉Kafka框架结构、分区分配策略、副本同步机制、Ack、幂等性、事务性，可根据数据流量灵活配置Kafka集群参数并扩展集群。
3. 熟悉HBase、Doris、StarRocks等实时分析数据库的整体架构、存储原理及应用场景，了解数据读写流程，能使用routine load实时摄取数据、建立物化视图加速查询，有大规模数据应用及性能调优经验。
4. 熟悉Hudi的设计原则，掌握COW和MOR两种表的使用及数据写入流程，能根据不同导入场景调优参数。
5. 具备Redis使用经验，了解基本数据类型的场景运用及运行参数调优。

### 数据可视化
拥有QuickBI可视化工具开发及使用经验。

### 查询工具
熟悉Impala、Presto、kylin等即席查询工具，具备实际应用经验。

### 脚本语言
熟悉Linux日常管理操作，掌握Linux/HDFS命令，能用Java/Python/Shell语言进行日常数据开发，具备良好的面向对象编程思想，熟悉JVM内存模型与垃圾回收流程，拥有JVM性能调优经验。

### 管理技能
具备团队管理经验（项目管理、技术方案设计、成员培养等），持有公司PMP项目管理认证。

## 项目经历
### 项目一：营赛洞察SaaS数据中台建设（数仓建设）
**项目架构**：Spark+Flink+Doris+华为云DLI(Hive)+华为云DataArts+Hudi+Kafka+matebase
**项目描述**：营赛洞察是东信时代的大数据SaaS产品，初期存在数据架构设计不合理、缺乏数据规范等问题，导致用户取数成本高、新指标开发耗时长、旧代码维护成本高、机器费用无序扩展。主导数仓架构升级，通过维度建模和业务域划分整合7大业务域（品牌、主体、商品、品类、活动、热点、KOL）解决数据孤岛问题；建设标准化报表体系提升数据应用效率；实现数据基建、数据资产、数据服务、数据应用的全链路改造升级。

**职责描述**：
1. 梳理业务场景，牵头完成数据指标梳理并构建指标体系，实现统一管理；
2. 负责大数据平台规划、架构设计及实施落地，为数据全流程提供架构支撑；
3. 设计、建模并持续改进数仓，利用维度建模构建高内聚、低耦合的数仓模型；
4. 建立数据开发规范，规范团队开发工作，保障代码提交质量；
5. 制定数据质量保障方案(SLA 监控)，保障公共层数据的及时性、稳定性和准确性；
6. 持续优化大数据平台离线、实时数据处理程序及架构，提升性能；
7. 预研大数据架构新技术并分析应用可行性；
8. 负责大数据团队的日常管理工作。

**项目成果**：
1. 作为数据组对外接口人，完成业务需求沟通与业务模型建设，构建标准化数仓，下线40+无用/临时数据表，整合20+应用层烟囱数据表，ODS穿透率从23%降至4%；
2. 制定设计规范与统一术语，完成数仓mapping建设和总线矩阵构建，优化100+烟囱式开发及高消耗任务，数据产出时效提前1小时；
3. 用Spark sql处理离线数据并完成Spark优化与版本升级至3.0，开启AQE实现任务自动优化，提升开发效率；
4. 基于Flink实现实时计算，完成事件表与维度表实时join，通过相关配置保证数据精准一次性消费，削峰填谷降低算法处理成本80%；
5. 引入Doris构建实时数仓，实现上百QPS查询并发和秒级查询速度，将定向监测模块时效性从小时级提升至分钟级；
6. 引入Hudi和华为云serverless服务，降低计算和存储成本约30%；
7. 通过metabase搭建自研OLAP分析系统，输出统一分析口径，节省数据开发导数时间。

### 项目二：营赛洞察SaaS数据治理平台（数据治理）
**项目架构**：Spark+Flink+华为云DLI(Hive)+华为云DataArts+python+shell
**项目描述**：随着业务发展，营赛洞察指标增多，数据分散、低质量、规范性和安全问题阻碍数据价值发掘。成立数据治理委员会，初期落实数据规范、提升数据质量、保障数据安全，解决数据孤岛；后期加强元数据管理，补齐数据属性信息，降低使用成本，打造标准化、安全可控、持续增值的数据资产。

**职责描述**：
1. 主导数据治理体系设计，搭建元数据管理平台及数据价值评价模型，实现元数据自动化采集与血缘分析；
2. 设计并开发实时数据质量监控系统，构建覆盖表级/字段级的6大质量维度监控体系；
3. 构建自动化检测任务，结合脚本、DQC工具减少人工干预；
4. 建立数据质量问题库，定期输出质量报告，集中管理历史问题；
5. 制定大数据任务管理规范及应急响应流程，确保异常及时解决；
6. 制定值班制度，明确表的分级，在保障数据产出的同时降低维护成本。

**项目成果**：
1. 构建标准化数据治理流程，明确各职能团队分工，提升组织协作效率；
2. 通过Python+Shell脚本集成华为云DataArts API，实现检测模板自动生成和质量规则库动态加载，企业微信告警，监控覆盖1000+数据表；
3. 梳理指标体系框架，定义原子/衍生指标，与产品经理共建指标体系，整合7大数据资产并实现标签及模型统一收口；
4. 制定数据开发SOP规范，设计P0/P1/P2分级保障机制，建立分钟级应急响应体系，关键任务SLA达标率提升至99.9%；
5. 打造全链路质量保障体系，实现核心数据资产100% DQC监控覆盖，建设7条基线保障机制，数据及时率从85%提升至99.7%，故障恢复时效缩短至30分钟；
6. 制定异常处理手册和值班规范，实现数据异常下游链路阻断与人工干预，问题平均响应时间从12小时缩短至1小时；
7. 管理数据质量问题，沉淀200+质量问题案例库，推动业务反馈量同比下降90%，数据信任度提升40%。

### 项目三：定向实时舆情监测系统（实时数仓）
**项目架构**：Flink+Kafka+Doris+Mysql+K8S+StreamPark+prometheus+grafana
**项目描述**：为赋能客户精准捕捉产品市场动态与投放活动即时反馈打造该系统，集成实时数据采集、情感分析、分词标注等算法，助力客户从多维度剖析品牌表现，实现市场响应即时化与精准化。

**职责描述**：
1. 负责项目规划与管理，全程把控立项到交付，定期组织周会，管理风险并协调多部门协同；
2. 主导前期可行性研究与方案设计，构建以Flink+Kafka+Doris为核心的高性能实时处理架构；
3. 研究并实践Flink on K8S部署方案，定制Flink镜像，依托华为云提升系统部署灵活性与可扩展性；
4. 开发核心数据流，实现发文、作者、评论、活动信息的深度关联与宽表加工；
5. 搭建StreamPark实时任务处理平台，维护任务运行并持续优化。

**项目成果**：
1. 采用K8S部署策略，降低服务器扩容与部署成本，依托其高可用特性保障任务稳定运行；
2. 运用Flink的Interval join和lookup join提升数据关联性能，通过Flink调优提升任务吞吐量；
3. 基于StreamPark搭建的平台实现60+实时任务全生命周期监控，集成双通道告警，任务异常恢复时效缩短至5分钟；
4. 通过prometheus + grafana实现30+ Kafka topic监控报警，数据丢失率降至0.01%；
5. 系统关键指标（舆情预警、情感分析）产出延迟从小时级压缩至20秒内，支撑客户营销策略调整效率提升5倍。

### 项目四：营赛洞见大模型（大模型知识库）
**项目架构**：FastAPI+SSE+华为云DLI+ES+Langchain
**项目描述**：面向智能营销场景的行业级大模型解决方案，集成LLM微调（LoRA）、RAG增强检索、多模态知识图谱（覆盖图文/视频3类媒介）等核心技术，支持12项核心业务场景，通过AI提升企业营销决策效率。

**职责描述**：
1. 构建模型服务，参考Langchain打通知识库、实时检索、Text2API等服务并上线维护；
2. 构建并优化知识库服务，通过多模态数据向量化技术实现数据与行业知识的高效关联与检索，优化数据结构与查询算法；
3. 构建内容审核模块，过滤不合规范的问答与回答；
4. 准备训练数据，为模型预训、微调、知识库提供数据支撑。

**项目成果**：
1. 通过多种技术屏蔽不当内容，助力模型通过信通院评测并成功上线；
2. 利用MinHash-LSH去重，通过规则和模型过滤低质量内容，确保合规性；
3. 通过多种手段完成品牌、品类、商品、景点4类知识的注入训练，知识完整度达85%；
4. 构建风控、爆文、品牌及图、文、视频三大知识库，支撑模型日常推理与风控工作。

### 项目五：大宇无限数据管理平台（数据平台）
**项目架构**：Athena+BigQuery+StarRocks+Flink+Hive+Superset
**项目描述**：旨在替换原有神策服务，基于Kimball建模理论构建数据仓库实现数据标准化管理，利用Flink做实时数据处理、Spark做离线处理，StarRocks实时摄取Kafka数据生成实时指标，通过Hudi和S3构建数据湖，Athena查询数据湖数据，Superset做数据可视化，基于AWS实现资源动态伸缩，支持大规模数据处理与分析。

**职责描述**：
1. 优化离线、实时数据处理与指标，保障数据的实时性与准确性；
2. 利用Flink CDC + Kafka框架实现10+数据源的无缝接入；
3. 搭建并优化StarRocks集群，基于其构建OLAP数据分析体系；
4. 搭建广告领域数据仓库，提供上百个高可靠业务指标，支撑多部门需求；
5. 利用Superset集成多数据源构建可视化平台，为多部门提供数据服务。

**项目成果**：
1. 参与数仓建模，采用Lambda架构和五层设计，运用星座模型做维度建模，模型落地多业务线并支撑业务决策；
2. 设计并落地公司实时数仓架构，实现实时数据处理与可视化；
3. 优化StarRocks分布式集群，将复杂查询响应时间从10秒优化至0.8秒；
4. 数据中台支持千万DAU用户及超百亿数据量，为多部门提供高可靠服务，提升查询速度并降低机器使用与维护成本。

## 自我评价
1. 具备良好的成本管理意识，能在成本消耗与项目目标间找到平衡点；
2. 拥有出色的沟通技巧，能快速、准确把握业务方的需求和意图；
3. 工作热情积极，乐于参与讨论并提出建设性意见；
4. 面对新技术和新挑战保持开放心态，乐于学习与探索；
5. 工作认真负责，确保在规定时间内高质量完成上级交办的任务；
6. 具备良好的团队合作精神，能有效组织协调团队资源，推动项目顺利开展。