# 深度学习基础
- 作为面试官的一点感受
- 面试考察什么
- 候选人越来越多，但招不到合适的人
- 大厂背景 ≠ 高能力
- 被奇葩候选人摧残的每一天
- 关于学历
- 有了LLM，机器学习/深度学习基础是否依然重要？
- 有了LLM，常规深度学习模型是否仍然需要掌握？
- 对于一个模型我们需要掌握到什么程度？
- 对于简历中的一个项目我们要掌握到什么程度？
- 对于学员的一些建议

## 目录
1. [深度学习基本概念](#1-深度学习基本概念)
2. [深度学习经典模型](#2-深度学习经典模型)
3. [深度学习优化策略](#3-深度学习优化策略)
4. [深度学习GPU使用原理](#4-深度学习gpu使用原理)
5. [深度学习模型训练策略](#5-深度学习模型训练策略)

---

# 1. 深度学习基本概念
## 初识机器学习
### 核心关联
人工智能 → 机器学习 → 深度学习 → LLM

### 学习与训练逻辑
- 学习：输入（知识/经历/历史数据）→ 认知 → 输出（新的问题答案）
- 训练：输入（新的数据）→ 最优解 → 输出（预测值）

### 模型与训练定义
模型是包含大量未知参数的函数，训练是通过大量数据迭代逼近未知参数最优解的过程。

### 数学示例
单变量线性模型：
$$ y = ax + b $$
$$
\to ax + b = y \to \begin{cases} a + b = 3 \\ 2a + b = 5 \end{cases} \to \begin{cases} a = 2 \\ b = 1 \end{cases} \to 2x + 1 = y
$$

多变量线性模型：
$$ y = a_1 x_1 + a_2 x_2 + a_3 x_3 + \dots = y \to 0.398 x_1 + 1.9 x_2 + 3.45 x_3 + \dots = y $$

### 关键定义
- 机器学习：研究计算机模拟人类学习行为，获取新技能、改善自身性能的学科，即“从样本中学习的智能程序”。
- 深度学习：源于人工神经网络研究，模拟人脑机制解释图像、声音、文本等数据的机器学习新领域。
- 核心共性：通过学习大量数据掌握分布规律，实现对同类数据的准确预测。

## 机器学习的两种典型任务
### 分类任务
- 定义：对离散值预测，根据样本特征判断所属类别（如A、B、C类），例：情感分类、内容审核。
- 本质：学习分类边界（决策边界），区分不同类别数据。
- 输出空间：非度量空间，仅关注“分类正确/错误”，无误差大小概念。

### 回归任务
- 定义：对连续值预测，根据样本特征输出具体数值，例：房价预测、股票预测。
- 本质：学习数据背后的分布规律，预测数据取值。
- 输出空间：度量空间，需衡量预测值与真实值的“误差大小”。

### 数学区分
$$ f(x) \to y, \quad x \in A, y \in B $$
- 分类任务：B为非度量空间（定性）
- 回归任务：B为度量空间（定量）

## 机器学习分类
### 有监督学习
- 核心：依赖大量标注数据，通过预测值与真实标签的误差反向传播（计算梯度、更新参数）优化模型。
- 特点：每条数据均有“正确答案”。

### 无监督学习
- 核心：不依赖标签，挖掘数据内在特征与样本间关系。
- 特点：只有数据无答案，常见任务如聚类（通过样本距离划分类别）。

### 半监督学习
- 核心：结合有标签数据（少量）与无标签数据（大量）训练模型。
- 示例：用有标签数据训练后，对无标签数据分类，再用正确分类的无标签数据继续训练。

### 自监督学习
- 核心：标注数据源于数据本身，无需人工标注。
- 应用：主流大模型预训练（如完形填空形式）。

### 远程监督学习
- 核心：用于关系抽取，基于已知三元组在文本中寻找共现句，自动生成有标签数据进行有监督学习。

### 强化学习
- 核心：智能体通过尝试不同行为，依据环境反馈的奖赏优化行为策略，以获取更高奖赏为目标。

## 基础名词解释
### 数据相关
- 样本：一条数据。
- 特征：被观测对象的可测量特性（如西瓜的颜色、瓜蒂、纹路）。
- 特征向量：用d维向量表征样本的全部或部分特征。
- 标签（label）/真实值：样本特征对应的真实类别或取值（正确答案）。
- 数据集（dataset）：多条样本组成的集合。
- 训练集（train）：用于训练模型的数据集合。
- 评估集（eval）：训练过程中周期性评估模型效果的数据集合。
- 测试集（test）：训练完成后评估最终模型效果的数据集合。

### 模型相关
- 模型：从数据中学习到的，实现特定功能/映射的函数。
- 误差/损失：样本真实值与预测值的差异。
- 损失函数：计算真实值与预测值误差的非负实值函数（又称目标函数）。
- 预测值：样本输入模型后的输出结果。
- 模型训练：用训练集迭代更新模型参数的过程。
- 模型收敛：预测结果与真实标签的误差稳定。
- 模型评估：用测试数据和评估指标检验模型效果的过程。
- 模型推理/预测：用训练好的模型对新数据进行预测。
- 模型部署：加载训练好的模型，对外提供推理服务。

## 机器学习任务流程
1. 明确任务 → 2. 模型选择 → 3. 数据集处理（Labeled Dataset）→ 4. 划分训练集（80%）与测试集（20%）→ 5. 模型训练（参数更新）→ 6. 模型评估（分类任务：Precision、Accuracy、Recall、F1；回归任务：MSE、RMSE、R2）→ 7. 模型部署与推理

## 模型训练详细流程
### 核心环节
- 前向传播（Forward propagation）：输入数据经多层网络计算输出预测值。
- 反向传播（Back propagation）：根据损失函数计算梯度，更新模型参数。
- 关键参数：
  - Step：一次梯度更新的过程。
  - Epoch：模型完成一次完整训练集训练的过程。
  - Batch：每次输入模型的数据子集（Batch size为单次输入数据量）。
  - LR（Learning Rate）：学习率，决定参数更新幅度。
  - Optimizer：优化器，指引参数更新方向与幅度。
  - Activation Function：激活函数，赋予模型非线性表达能力。

### 前向传播与反向传播核心公式
梯度计算：
$$
\begin{aligned} 
\frac{\partial L}{\partial w_i} &= \frac{\partial L}{\partial y'} \frac{\partial y'}{\partial z} \frac{\partial z}{\partial w_i} \\ 
&= \frac{\partial L}{\partial y'} \frac{\partial y'}{\partial (\sum w_i x_i)} \frac{\partial (\sum w_i x_i)}{\partial w_i} \\ 
&= \frac{\partial L}{\partial y'} \frac{\partial y'}{\partial z} x_i 
\end{aligned}
$$

参数更新：
$$ w_i = w_i - \alpha \frac{\partial L}{\partial w_i} $$

## 关键训练参数
### Learning Rate（学习率）
- 定义：决定模型参数更新幅度，LR越高更新越激进，反之越平缓。
- 影响：
  - 过小：Loss下降极慢，训练效率低。
  - 过大：参数更新幅度过大，导致振荡、收敛到局部最优或Loss上升。

### Batch size
- 定义：单次输入模型的数据数量。
- 核心影响：
  - 内存占用：Batch size越大，GPU显存占用越多，易导致OOM（内存不足）。
  - 训练效率：Batch size越大，单个Epoch所需迭代次数越少，训练越快。
  - 模型稳定性：合理范围内，Batch size越大，梯度计算越稳定，训练曲线越平滑。
  - 泛化能力：小Batch size易收敛到flat minimum（泛化能力好），大Batch size易收敛到sharp minimum（泛化能力差）。
- 选型建议：
  - 传统模型：较小Batch size性能更优。
  - 大模型：较大Batch size（需配合足够迭代次数）。
  - 硬件适配：根据GPU显存调整，避免OOM。

## 激活函数
### 核心作用
- 保证多层神经网络不退化为线性网络，赋予模型非线性表达能力，使其能逼近任意非线性函数。

### 常见激活函数
#### sigmoid
- 公式：$$ \text{sigmoid}(x) = \frac{1}{1 + e^{-x}} $$
- 特点：
  - 软饱和特性，正负饱和区梯度接近0，仅0附近激活效果好。
  - 导数值最大0.25，反向传播时每层至少75%损失，易导致梯度消失（5层内出现）。
  - 输出不以0为中心，参数更新效率低。
  - 涉及指数运算，计算速度慢。

#### softmax
- 公式：$$ \text{softmax}(z_j) = \frac{e^{z_j}}{\sum_{k=1}^{K} e^{z_k}} $$
- 应用：多分类任务，输出各类别概率分布（和为1）。

#### tanh
- 公式：$$ \text{tanh}(x) = \frac{2}{1 + e^{-2x}} - 1 $$
- 特点：输出以0为中心，缓解sigmoid的参数更新效率问题。

#### ReLU
- 公式：$$ \text{ReLU}(x) = \max(0, x) = \begin{cases} x & x \ge 0 \\ 0 & x < 0 \end{cases} $$
- 特点：
  - 分段线性函数，非线性表达能力。
  - 无梯度消失问题，计算成本低，收敛速度比sigmoid快6倍。
  - 存在dead ReLU问题（输入为负时输出0，梯度无法流动，权重不更新）。

#### Leaky ReLU
- 公式：$$ \text{Leaky ReLU}(x) = \begin{cases} x & x > 0 \\ \gamma x & x \le 0 \end{cases} $$
- 改进：解决dead ReLU问题（负输入时输出非零）。

#### ELU
- 公式：$$ \text{ELU}(x) = \begin{cases} x & x > 0 \\ \alpha(e^x - 1) & x \le 0 \end{cases} $$
- 特点：结合ReLU与tanh优势，抗噪声能力强。

#### Swish
- 公式：$$ \text{Swish}(x) = x \cdot \text{sigmoid}(\beta x) = \frac{x}{1 + e^{-\beta x}} $$
- 特点：
  - 无上界（无梯度饱和），有下界（无dead ReLU）。
  - 处处连续可导，替换ReLU可提升模型性能。

### 激活函数输出均值为0的意义
- 若输出仅为正值（如sigmoid），则第i层所有参数\(w_{i}\)更新方向一致，模型需走Z字形逼近最优解，效率低下。
- 均值为0可使参数更新方向多样化，提升收敛效率。

## 损失函数
### 核心定义
- 作用：度量模型预测值\(f(x)\)与真实值\(Y\)的差异程度，仅用于训练阶段。
- 训练目标：通过更新参数，使损失函数逼近全局最小值。
- 任务适配：不同任务对应不同损失函数（回归：MAE、MSE；分类：交叉熵损失）。

### 常见损失函数
#### MSE（均方误差）
- 公式：$$ \text{MSE} = \frac{1}{m} \sum_{i=1}^{m} (y_i - \hat{y}_i)^2 $$
- 别名：平方损失、L2损失。
- 思想：使训练点到最优拟合线的平方和最小，适用于回归任务。
- 缺点：量纲与数据不一致，对异常值敏感。

#### MAE（平均绝对误差）
- 公式：$$ \text{MAE} = \frac{1}{m} \sum_{i=1}^{m} |y_i - \hat{y}_i| $$
- 别名：L1损失。
- 特点：量纲与数据一致，反映真实误差，对异常值鲁棒性强。

#### 交叉熵损失
##### 二分类
- 公式：$$ L = -\frac{1}{N} \sum_{i} [y_i \log p_i + (1 - y_i) \log(1 - p_i)] $$
- 变量说明：$y_i$为真实标签（正类1，负类0），$p_i$为预测正类概率。
- 示例：正样本A预测正类概率0.8时，损失 $= -\log(0.8) = 0.0969$；概率0.5时，损失 $= -\log(0.5) = 0.3010$（误差越大，损失越大）。

##### 多分类
- 公式：$$ L = -\frac{1}{N} \sum_{i} \sum_{c=1}^{M} y_{ic} \log(p_{ic}) $$
- 变量说明：$M$为类别数，$y_{ic}$为符号函数（真实类别为c时1，否则0），$p_{ic}$为预测类别c的概率。

##### 激活函数适配
- 二分类：sigmoid或softmax。
- 单标签多分类：softmax（互斥输出）。
- 多标签多分类：sigmoid。

# 2. 深度学习经典模型
## RNN（Recurrent Neural Network）
### 核心公式
$$ O_t = g(V \cdot S_t) $$
$$ S_t = f(U \cdot X_t + W \cdot S_{t-1}) $$

### 关键特点
- 与传统神经网络的区别：将前一次输出结果带入下一次隐藏层共同训练，所有历史输入均影响未来输出。
- 缺陷：
  - 短期记忆影响大，长期记忆衰减（短期记忆问题）。
  - 无法并行训练，训练成本高。

## LSTM（Long Short-Term Memory）
### 核心改进
- 在输入、输出、隐含层自循环基础上，增加三个门单元，解决RNN长期记忆问题：
  - 遗忘门：控制对细胞内部状态的遗忘程度。
  - 输入门：控制对细胞输入的接收程度。
  - 输出门：控制对细胞输出的认可程度。
- 门单元控制：由当前输入信号和隐含层前一时刻输出共同决定。

### 核心公式
#### 门单元
$$ f_t = \text{sigmoid}(b_f + U_f x_t + W_f h_{t-1}) $$
$$ i_t = \text{sigmoid}(b_{in} + U_{in} x_t + W_{in} h_{t-1}) $$
$$ o_t = \text{sigmoid}(b_o + U_o x_t + W_o h_{t-1}) $$

#### 候选记忆与细胞状态
$$ c_t = \text{tanh}(b + U x_t + W h_{t-1}) $$
$$ s_t = f_t \otimes s_{t-1} + i_t \otimes c_t $$

#### 输出计算
$$ h_t = o_t \otimes \text{tanh}(s_t) $$
$$ z_t = \text{softmax}(V h_t + c) $$

## Seq2Seq
### 核心结构
- 由编码器（Encoder）和解码器（Decoder）组成，适用于序列转换任务（如机器翻译、对话生成）。
- 关键改进：引入Attention机制，提升长序列建模能力。

## Attention机制
### 核心思想
- Decoder每一步解码时，对Encoder输入序列的所有隐层信息 $h_1, h_2, \dots, h_{T_x}$ 进行加权求和，聚焦与当前预测相关的输入信息。

### 核心公式
$$ c_i = \sum_{j=1}^{T_x} a_{ij} h_j $$
$$ s_t = f(s_{t-1}, y_{t-1}, c_t) $$

### 优势
- 解决Seq2Seq模型中长距离依赖建模问题，提升序列转换任务性能。

## Transformer
### 核心结构
#### Encoder
- 由6个相同层堆叠而成，每层包含：
  1. 多头自注意力机制（Multi-Head Self-Attention）。
  2. 位置-wise全连接前馈网络（Feed-Forward Network）。
- 残差连接（Residual Connection）+ 层归一化（Layer Normalization）：\(LayerNorm(x + Sublayer(x))\)。
- 输出维度：\(d_{model}=512\)。

#### Decoder
- 由6个相同层堆叠而成，每层包含：
  1. 掩码多头自注意力机制（Masked Multi-Head Self-Attention）。
  2. 多头注意力机制（关注Encoder输出）。
  3. 位置-wise全连接前馈网络。
- 残差连接+层归一化，掩码机制防止位置i关注后续位置信息。

### 输入处理
#### 词嵌入（Token Embedding）
- 将每个token转换为固定维度向量（如512维）。

#### 位置编码（Position Embedding）
- 必要性：Transformer无循环结构，需通过位置编码注入词序信息。
- 实现方式：相对位置编码（$\sin/\cos$ 函数），满足：
  1. 区分不同位置词汇。
  2. 体现词汇先后次序，编码值不依赖文本长度。
  3. 值域范围限制（$[-1, 1]$）。
- 公式：
$$ PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d_{model}}) $$
$$ PE_{(pos, 2i+1)} = \cos(pos / 10000^{2i/d_{model}}) $$

### 多头自注意力机制
#### 核心逻辑
- 将Q、K、V通过线性变换后切分为H份（多头），在每个低维空间独立计算自注意力，最后拼接输出。
- 优势：捕捉不同维度的语义关联。

#### 核心公式
$$ \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V $$

#### 维度转换示例
$$ [\text{batch\_size}, \text{seq\_len}, 512] \to [\text{batch\_size}, \text{seq\_len}, 12, 64] \to [\text{batch\_size}, 12, \text{seq\_len}, 64] $$
（12头，每头维度64，$512 = 12 \times 64$）

### 前馈网络（Feed-Forward）
- 公式：$$ \max(0, XW_1 + b_1)W_2 + b_2 $$
- 结构：两层全连接，第一层激活函数为ReLU，第二层无激活函数。

### 关键优势
- 并行计算能力强（无循环依赖）。
- 长距离依赖建模效果好（Attention机制）。

## BERT（Bidirectional Encoder Representation from Transformers）
### 核心特点
- Pre-training：大量数据预训练通用模型，支持下游任务微调。
- Deep：BERT-base含12层Encoder。
- Bidirectional：通过MLM任务实现上下文双向理解。
- Transformer：基于Transformer Encoder。
- Self-supervised Learning：无监督数据训练（MLM+NSP任务）。

### 预训练任务
#### MLM（Masked Language Model）
- 流程：以15%概率随机选中token，按以下方式处理后预测：
  1. 80%概率替换为[MASK]。
  2. 10%概率替换为随机token。
  3. 10%概率保持不变。

#### NSP（Next Sentence Prediction）
- 流程：
  1. 50%概率保留句子对原有顺序（标注IsNext）。
  2. 50%概率替换后句为随机句子（标注NotNext）。
- 目标：用[CLS] token的输出进行二分类，学习句子间逻辑关系。

### 输入结构
- Token Embeddings：token→768维向量（look up）。
- Segment Embeddings：区分双句输入（0/1）。
- Position Embeddings：训练式位置编码（look up）。

### 模型规格（BERT-base）
- 层数：12
- 最大序列长度：512
- 隐藏层维度：768
- 头数：12
- 参数量：110M

### 下游任务适配
- 句对分类任务（如MNLI、QQP）。
- 单句分类任务（如SST-2、CoLA）。
- QA任务（如SQuAD）。
- NER任务（如CoNLL-2003）。

## RoBERTa
### 核心改进
- 舍弃NSP任务，延长序列长度。
- 增加预训练数据（16GB→160GB）。
- 增大batch_size（256→8K）。
- 扩大词表（30K→50K）。
- 动态掩码：训练阶段实时计算掩码，提升数据复用效率。

## ALBERT
### 核心改进
- 词向量因式分解：解耦Embedding维度E与Transformer隐层维度H，降低计算复杂度（VH→VE+EH）。
- Transformer跨层参数共享：每层参数相同，减少参数总量。
- SOP（Sentence Order Prediction）任务替代NSP：判断句子对是否颠倒，提升语义理解能力。

## T5（Transfer Text-to-Text Transformer）
### 核心思想
- 将所有NLP任务转化为Text-to-Text（文本到文本）任务，例：
  - 翻译：“translate English to German: That is good.” → “Das ist gut.”
  - 摘要：“summarize: state authorities dispatched emergency crews...” → “six people hospitalized after a storm...”

### 架构选择
- Encoder-Decoder：双向注意力（Encoder）+ 单向注意力（Decoder），效果最优。
- 其他架构：Decoder-only（如GPT）、Prefix LM（混合双向/单向注意力）。

### 预训练目标（Objectives）
#### 高层次方法
1. Prefix LM：有条件文本生成（输入完整文本，从左到右预测）。
2. BERT-style：破坏文本后还原。
3. Deshuffling：打乱文本后还原。

#### 破坏策略
1. Mask：替换为特殊符（如[M]）。
2. Replace spans：相邻破坏token合并为单个特殊符。
3. Drop：直接丢弃破坏token。

#### 最优参数
- 破坏比例：15%（与BERT一致）。
- Span长度：3。

### 模型规格
| 模型大小 | 参数量 | 编码器/解码器层数 | 隐藏层维度 | 头数 |
|----------|--------|------------------|------------|------|
| Small    | 60M    | 6                | 512        | 8    |
| Base     | 220M   | 12               | 768        | 12   |
| Large    | 770M   | 24               | 1024       | 16   |
| 3B       | 2.8B   | 24               | 1024       | 32   |
| 11B      | 11B    | 24               | 1024       | 128  |

# 3. 深度学习优化策略
## 优化器
### 核心定义
- 作用：反向传播中指引参数更新方向与幅度，使损失函数逼近全局最小值。
- 本质：梯度的“更新者”，不计算梯度。

### 梯度下降算法
#### 核心公式
$$ w_i = w_i - \alpha \frac{\partial L}{\partial w_i} $$

#### 三种形式
1. BGD（批量梯度下降）：
   - 特点：全量数据计算梯度，准确但速度慢、内存开销大，不适用于大数据/大模型。
2. SGD（随机梯度下降）：
   - 特点：单样本估计梯度，速度快、内存开销小，但梯度估计偏差大，收敛不稳定（振荡）。
3. Mini-Batch GD：
   - 特点：批量子集计算梯度，平衡BGD与SGD优势，为当前主流选择。

### 常见优化器
#### Momentum（动量）
- 核心思想：引入惯性保持，模拟物理动量，加速收敛、抑制振荡。
- 公式：
$$ v_t = \gamma v_{t-1} + \eta g_t $$
$$ \theta_{t+1} = \theta_t - v_t $$
- 变量说明：$\gamma$ 为衰减系数（阻力），$\eta$ 为学习率，$g_t$ 为当前梯度。

#### AdaGrad
- 核心思想：自适应学习率，频繁更新参数的学习率衰减更快。
- 公式：
$$ \theta_{t+1, i} = \theta_{t, i} - \frac{\eta}{\sqrt{\sum_{k=0}^{t} g_{k, i}^2 + \epsilon}} g_{k, i} $$
- 缺陷：学习率衰减过于激进，后期训练停滞。

#### RMSprop
- 核心思想：改进AdaGrad，用梯度平方的指数加权移动平均值替代历史总和，缓解学习率过快衰减。
- 公式：
$$ s_t = \gamma \cdot s_{t-1} + (1-\gamma) \cdot g_{t, i}^2 $$
$$ \theta_{t+1, i} = \theta_{t, i} - \frac{\eta}{\sqrt{s_t + \epsilon}} g_{t, i} $$
- 推荐参数：$\gamma = 0.9$，$\eta = 0.001$。

#### Adam
- 核心思想：融合Momentum（惯性保持）与AdaGrad（自适应学习率）优势。
- 公式：
  - 一阶矩（动量）：$$ m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t $$
  - 二阶矩（自适应）：$$ v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2 $$
  - 偏差矫正：$$ \hat{m}_t = \frac{m_t}{1 - \beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1 - \beta_2^t} $$
  - 参数更新：$$ \theta_{t+1} = \theta_t - \frac{\eta \cdot \hat{m}_t}{\sqrt{\hat{v}_t + \epsilon}} $$
- 推荐参数：$\beta_1 = 0.9$，$\beta_2 = 0.999$。

## 模型评估指标
### 分类模型指标
#### Accuracy（准确率）
- 公式：$$ \text{Accuracy} = \frac{n_{\text{correct}}}{n_{\text{total}}} $$
- 缺陷：样本不均衡时失效（如负样本占99%，全预测为负样本仍得99%准确率）。

#### 混淆矩阵（Confusion Matrix）
- 核心指标：
  - TP（True Positive）：正类预测为正类。
  - FN（False Negative）：正类预测为负类。
  - FP（False Positive）：负类预测为正类。
  - TN（True Negative）：负类预测为负类。

#### Precision（精准率）
- 公式：$$ \text{Precision} = \frac{TP}{TP + FP} $$
- 含义：预测为正样本的样本中，实际为正样本的概率。

#### Recall（召回率）
- 公式：$$ \text{Recall} = \frac{TP}{TP + FN} $$
- 含义：实际为正样本的样本中，被正确预测为正样本的概率。

#### F1-Score
- 公式：$$ F1 = \frac{2 \times \text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}} $$
- 含义：Precision与Recall的调和平均，综合两者性能。
- 变体：
  - Micro-F1：不区分类别，用总体样本计算。
  - Macro-F1：计算每个类别F1后求均值。
  - 选型：数据均衡任选；不均衡且差异大 → Macro-F1；不均衡且差异小 → Micro-F1。

### 回归模型指标
#### MSE（均方误差）
- 公式：$$ \text{MSE} = \frac{1}{m} \sum_{i=1}^{m} (y_i - \hat{y}_i)^2 $$
- 缺陷：量纲与数据不一致，放大异常值影响。

#### RMSE（均方根误差）
- 公式：$$ \text{RMSE} = \sqrt{\frac{1}{m} \sum_{i=1}^{m} (y_i - \hat{y}_i)^2} $$
- 改进：解决MSE量纲不一致问题，仍对异常值敏感。

#### MAE（平均绝对误差）
- 公式：$$ \text{MAE} = \frac{1}{m} \sum_{i=1}^{m} |y_i - \hat{y}_i| $$
- 特点：量纲与数据一致，反映真实误差，对异常值鲁棒。

#### R²（决定系数）
- 公式：$$ R^2 = 1 - \frac{\sum_i (\hat{y}_i - y_i)^2}{\sum_i (\bar{y}_i - y_i)^2} $$
- 含义：衡量模型拟合效果，取值范围 $[0, 1]$，越接近1拟合越好。
- 缺陷：样本数量增加时必然上升，无法准确定量准确率。

### 其他指标
#### GSB
- 公式：$$ \text{GSB} = \frac{\text{good} - \text{bad}}{\text{good} + \text{same} + \text{bad}} $$
- 用途：对比两个模型在某类数据中的性能差异。

## 学习率调度策略（Learning Rate Scheduler）
### 核心思想
- 训练初期用较大LR快速更新参数，后期逐步降低LR，使模型收敛到最优解。

### 常见策略
- StepLR：按固定epoch间隔衰减LR。
- ExponentialLR：按指数规律衰减LR。
- MultiStepLR：按指定epoch节点衰减LR。
- CosineAnnealingLR：按余弦曲线周期性调整LR。

## Warm up（预热）
### 核心作用
- 训练初期模型权重随机初始化或与数据分布差异大，大LR易导致振荡，Warm up以小LR逐步稳定模型，再切换至预设LR训练。
- 优势：
  - 缓解初始阶段过拟合。
  - 保持模型深层稳定性。

### 典型流程
- 先从极小LR逐步增大（Warm up阶段），完成后按LR decay策略训练（如微调LLaMA-7B）。

## Batch size与学习率适配
### 核心原则
- 传统模型：较小Batch size+适配LR，避免过拟合。
- 大模型：较大Batch size+等比例增大LR（Linear Scaling LR），保证收敛稳定性。
- 关键结论：
  - Batch size过大不直接导致性能下降，核心是迭代次数不足。
  - 固定LR下存在最优Batch size，需结合硬件与任务调整。
  - 可通过增大Batch size替代LR衰减（部分场景有效）。

# 4. 深度学习GPU使用原理
## 数据精度
### 常见精度类型
| 类型 | 指数位 | 小数位 | 核心特点 |
|------|--------|--------|----------|
| FP32（单精度） | 8bit | 23bit | 精度高，内存占用大 |
| FP16（半精度） | 5bit | 10bit | 内存占用仅FP32的1/2，计算快 |
| BF16（脑半精度） | 8bit | 7bit | 截断FP32，兼顾范围与速度 |

### 半精度（FP16）的优势
- 内存占用少：支持更大Batch size，减少GPU并行通信量。
- 计算更快：主流GPU优化后，吞吐量为FP32的2-8倍。
- 大模型微调基础：降低显存压力。

### 半精度的问题
- 下溢出：FP16表示范围（6×10⁻⁸~65504）远小于FP32（1.4×10⁻⁴⁵~1.7×10³⁸），易出现数值溢出。
- 舍入误差：梯度小于FP16最小间隔时被忽略（如[2⁻³,2⁻²]区间间隔为2⁻¹³，梯度<2⁻¹³时失效）。

## CUDA基本概念
### 线程层次结构
- 线程（Thread）：CUDA基本执行单元，硬件支持、开销小，执行相同代码。
- 线程块（Block）：若干线程的分组，支持1D/2D/3D结构，单块最多512/1024个线程（依GPU规格）。
- 线程网络（Grid）：若干Block组成的网格，支持1D/2D结构。

### GPU内存层次
- 寄存器（Register）：最快，线程私有。
- 共享内存（Shared Memory）：Block内线程共享，速度快（SM一级缓存）。
- 全局内存（Global Memory）：CPU与GPU共享，速度较慢。
- 常量内存（Constant Memory）、纹理内存（Texture Memory）：特定场景优化内存。

### GPU执行机制
- 计算核心（SM，Streaming Multiprocessor）：包含运算单元、寄存器、缓存，是执行核心。
- Warp：SM一次执行的线程组（32/64个线程），Block绑定到SM上，按Warp分批执行。
- 示例：SM支持64线程/Warp，1024线程/Block需分16次执行。

### Fermi架构关键参数（示例）
- 每SM最多8个Block。
- 每SM最多48个Warp。
- 每SM最多1536个线程（48×32）。

## GPU并行方式
### 数据并行（Data Parallelism）
- 核心：模型复制到每个GPU，每个GPU处理不同数据子集，计算梯度后求平均，同步更新参数。
- 本质：增大Batch size，提升训练速度。
- 优势：Pytorch原生支持，是最常用的并行方式。
- 同步策略：
  1. 单GPU计算梯度。
  2. 计算全局平均梯度（阻塞，数据传输影响速度）。
  3. 各GPU独立更新参数。

### 流水并行（Pipeline Parallelism）
- 核心：模型按层分割，不同层部署在不同GPU，按顺序执行前向/反向传播。
- 适用场景：GPU显存不足，无法容纳完整模型。
- 缺陷：层间依赖导致“气泡”（空闲时间），朴素实现效率低。
- 优化方案：PipeDream等，重叠不同批次的计算与通信，减少空闲时间。

### 张量并行（Tensor Parallelism）
- 核心：拆分单个数学运算（如矩阵乘法）到多个GPU，分别计算后整合结果。
- 实现：将权重矩阵切分为“shards”，不同GPU负责不同分片，通过通信整合结果。
- 适用场景：大模型单层计算量巨大（如Transformer的Attention层）。

### 混合专家系统（Mixture-of-Experts, MoE）
- 核心：输入仅触发模型部分“专家”（子网络）计算，不增加计算成本的同时扩充参数。
- 结构：
  - 知识库：各专家的专属知识与规则。
  - 推理引擎：根据输入调用适配专家。
  - 融合模块：加权平均、投票等方式整合专家输出。
- 优势：动态选择专家，适配不同任务，支持大规模GPU扩展。

## Pytorch DDP（Distributed Data Parallel）
### 核心原理
- 启动N个进程（N=GPU数量），每个进程在对应GPU加载模型（参数初始值一致）。
- 训练时通过Ring-Reduce算法交换梯度，各进程获取全局平均梯度后更新参数。
- 通信特点：进程仅与左右邻居通信，降低传输开销。

### 并行训练通讯
#### Ring Allreduce
- 核心逻辑：
  1. Scatter Reduce：每个卡向邻居发送数据并累加，最终每张卡获得梯度的一部分完整和。
  2. Allgather：将完整部分分发回所有卡。
- 效率：通讯量与显卡数量无关，仅与模型参数量相关。
- 通讯量计算公式：
$$ \text{Total Data} = 2 \times (N - 1) \times \frac{K}{N} $$
- 变量说明：$N$ 为显卡数，$K$ 为参数总量。

## GPU训练效率对比
| 硬件配置 | 模型（BERT-base） | 训练数据集（180000条） | Batch size=32时训练时间 |
|----------|-------------------|------------------------|-------------------------|
| CPU（Xeon Gold 5118，2CPU24Core） | BERT-base | 180000 | 40:30:00 |
| CPU（Xeon Platinum 8255C，1CPU40Core） | BERT-base | 180000 | 16:25:00 |
| GPU（P40） | BERT-base | 180000 | 3:00:00 |
| GPU（V100） | BERT-base | 180000 | 1:17:00 |
| 2×GPU（V100） | BERT-base | 180000 | 0:39:00 |
| 3×GPU（V100） | BERT-base | 180000 | 0:18:00 |
| 4×GPU（V100） | BERT-base | 180000 | 0:07:00 |

# 5. 深度学习模型训练策略
## 核心优化方向
- 并行训练：数据并行、流水并行、张量并行、MoE并行，提升训练速度与规模。
- 精度优化：采用FP16/BF16，平衡速度与精度，降低显存占用。
- 参数调优：合理设置Batch size、LR、优化器，提升收敛效率与模型性能。
- 硬件适配：基于GPU显存与计算能力，调整训练参数（如Batch size、精度）。

## 关键实践建议
1. 大模型训练：优先采用数据并行+张量并行，配合FP16/BF16精度，设置较大Batch size（如8K/16K）。
2. 显存优化：使用梯度检查点（Gradient Checkpointing）、模型并行，避免OOM。
3. 收敛稳定性：采用Warm up+CosineAnnealingLR，配合Adam优化器。
4. 泛化能力：合理划分训练/评估/测试集，使用正则化（Dropout、Weight Decay）。

需要我帮你将这份MD文件导出为可直接保存的格式，或者补充公式的渲染说明吗？