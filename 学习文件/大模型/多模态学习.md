# 多模态技术
## 一、多模态技术概述
### 1. 什么是模态？
模态（Modality）是德国理学家赫尔姆霍茨提出的生物学概念，指生物凭借感知器官与经验接收信息的通道，如人类的视觉、听觉、触觉、味觉和嗅觉模态。

### 2. 什么是多模态？
多模态是融合语言、图像、音频、视频、手势等多种不同类型信息模式的形式，这些模式相互协作补充，可提供更全面、丰富且准确的信息表达与理解。

在多模态研究中，核心是整合处理多模态信息，挖掘其关联与协同关系，以实现更智能高效的人机交互和信息处理，广泛应用于图文结合理解、音视频分析融合、多感官体验整合等领域，能模拟人类感知认知方式，提升系统性能与智能化水平。

### 3. 典型应用场景
- 视觉问答：引入图像信息，让模型理解图像内容并回答用户问题。
- 音频对话系统：模型通过多模态学习接收理解用户语音输入，再合成语音回复。
- 多模态推理：结合图像、文本等多模态信息，对复杂场景进行推理分析。

## 二、多模态模型
### 1. CNN（卷积神经网络）
- 核心流程：包含卷积（Convolutions）、下采样（Subsampling）、全连接（Full connection）、高斯连接（Gaussian connections）等操作。
- 输入输出示例：$32 \times 32$ 输入，经多层处理后输出 $10$ 维结果，用于分类等任务。
- 关键特性：具有局部性（Locality）和平移等变性（Translation Equivariance）两大归纳偏置，适合小数据集训练，但感受野有限，难以建模全局依赖。

### 2. ViT（Vision Transformer）
#### （1）核心定位
2020 年 Google 团队提出，将 Transformer 应用于图像分类领域，模型简单、效果好、可拓展性强，是 Transformer 在 CV 领域应用的里程碑。

#### （2）数据训练特性
- 中等数据集（如 ImageNet）无强正则化训练时，准确率低于同等规模 ResNet。
- 大规模数据集（14M-300M 图像）训练后，泛化能力显著提升，在 ImageNet、CIFAR-100 等多个基准测试中接近或超越 SOTA 水平（ImageNet 准确率 88.55%）。

#### （3）模型结构
1. Patch Embedding：将输入图像分割为固定大小 Patch（如 $16 \times 16$），$224 \times 224 \times 3$ 图像可生成 $196$ 个 Patch，每个 Patch 经线性投射后维度为 $768$，加入 [CLS] 标志位后最终维度为 $197 \times 768$。
2. Positional Encoding：采用可学习的 $1\text{-D}$ 位置编码，与 Patch 嵌入求和（非拼接），维度保持 $197 \times 768$。
3. Transformer Encoder：包含 LN（层归一化）、多头自注意力（Multi-Head Attention）、MLP 层，多头自注意力将输入映射为 $q、k、v$，输出维度维持 $197 \times 768$；MLP 层将维度放大至 $3072$ 再缩小回 $768$。
4. 模型表达式：
$$
\begin{aligned}
z_{0} &= [x_{\text{class}} ; x_{p}^{1} E ; x_{p}^{2} E ; \dots ; x_{p}^{N} E] + E_{\text{pos}}, & E \in \mathbb{R}^{(P^{2} \cdot C) \times D}, E_{\text{pos}} \in \mathbb{R}^{(N+1) \times D} \\
z_{\ell}^{\prime} &= \text{MSA}(\text{LN}(z_{\ell-1})) + z_{\ell-1}, & \ell = 1, \dots, L \\
z_{\ell} &= \text{MLP}(\text{LN}(z_{\ell}^{\prime})) + z_{\ell}^{\prime}, & \ell = 1, \dots, L \\
y &= \text{LN}(z_{L})
\end{aligned}
$$

#### （4）关键技巧（Tricks）
- Image Presentation：可使用全局平均池化（GAP）替代 [CLS] 标志位，性能接近。
- Positional Encoding：1-D、2-D、相对位置编码效果相近，甚至不使用位置编码性能损失也不大。
- Inductive Biases：缺乏 CNN 固有的归纳偏置，需依赖大规模数据训练以学习全局信息。
- Hybrid Architecture：先通过 CNN 提取特征图，拉直后加入 [CLS] 再输入 Patch Embedding 处理。

### 3. CLIP（Contrastive Language-Image Pre-training）
#### （1）核心定位
OpenAI 提出的图像+语言多模态模型，预训练任务为预测图文匹配关系，泛化性和鲁棒性优异，具有强大的零样本（zero-shot）能力，无需 ImageNet 数据集训练即可达到 ResNet50 有监督训练效果。

#### （2）模型结构
1. 双编码器：包含图像编码器（ResNet 或 ViT）和文本编码器（CBOW 或 Text Transformer）。
2. 预训练流程：通过对比学习最大化正样本图文特征的余弦相似度，最小化负样本相似度。
3. 图像分类流程：将类别名填入 Prompt Template（如“A photo of a {object}.”），生成文本嵌入，与图像嵌入计算余弦相似度，归类至相似度最高类别。

#### （3）关键技巧（Tricks）
- 数据集构建：创建含 $4$ 亿对（image, text）的 WIT 数据集，来源于互联网公开资源。
- 预训练方法：对比学习策略，联合优化图像和文本的交叉熵损失。
- Prompt Engineering：使用句子形式的 Prompt 避免歧义，采用 Prompt Ensembling 方法（$80$ 种不同 Prompt 集成结果）提升性能。

#### （4）实验结论
在 27 个数据集上与 ResNet 对比，多数任务表现更优（如 StanfordCars 提升 28.9%）；few-shot 模式下性能优于 zero-shot，全量数据训练时部分模型变体表现领先 EfficientNet L2 NS。

### 4. BLIP（Bootstrapping Language-Image Pre-training）
#### （1）核心定位
2022 年 Salesforce 提出的多模态框架，统一多模态理解与生成，引入跨模态编码器和解码器，通过 CapFilt 进行数据扩充与清洗，在多项视觉语言任务中取得 SOTA。

#### （2）模型结构
采用多模态混合结构 MED（Multimodal mixture of Encoder-Decoder），包含图像编码器、文本编码器、基于图像的文本编码器和基于图像的文本解码器，共享大量参数，训练时图像端一次前向运算对应文本端三次前向运算。

#### （3）关键技巧（Tricks）
- 预训练目标：联合优化图像-文本对比损失（ITC）、图像-文本匹配损失（ITM）、语言建模损失（LM）。
- CapFilt 机制：包含 Captioner（生成合成字幕）和 Filter（过滤噪声文本），两者均由预训练模型初始化并在人工标注数据集上微调，最终结合过滤后的图文对和人工标注数据重新预训练模型。

#### （4）实验结果
在 COCO、Flickr30K 数据集微调后，在图像字幕生成、视觉问答（VQA）等下游任务中表现优异，CapFilt 机制可稳定提升模型性能。

## 三、多模态大模型
### 1. GPT-4o (Omni)
#### （1）核心定位
2024 年 5 月发布的旗舰级全能（Omni）模型。首次实现端到端（End-to-End）训练，跨越音频、视觉和文本三种模态，能够实时进行多模态交互，响应速度接近人类（平均 320ms）。

#### （2）模型架构
- **端到端单一模型**：不同于以往级联（Cascading）模型（如：先语音转文字，再输入 LLM），GPT-4o 是一个单一神经网络，所有输入和输出都在同一模型中处理，减少了信息损失和延迟。
- **Tokenizer 优化**：引入全新的 Tokenizer，对非拉丁语系压缩率更高，推理更高效。

#### （3）关键特性
- **实时音频交互**：支持打断，能感知说话者的情绪并调整输出语调。
- **原生视觉理解**：在 MMMU、MathVista 等基准测试中表现卓越，能够实时理解摄像头视频流并进行对话。

### 2. Claude 3.5 Sonnet
#### （1）核心定位
Anthropic 于 2024 年 6 月发布。虽然仍是级联架构，但在逻辑推理、代码编写和**视觉理解**方面表现极为强悍，在多项指标上超越了 GPT-4o。

#### （2）视觉能力
- **复杂图表分析**：在处理复杂的财务报表、流程图和手写文本转录方面具有行业领先的准确度。
- **Artifacts 交互**：通过多模态理解生成的代码或 UI，可直接在侧边栏渲染，极大地提升了图文生成与交互的体验。

### 3. Gemini 1.5 Pro
#### （1）核心定位
Google DeepMind 开发的高性能多模态模型，核心优势在于**超长上下文（Long Context）**和原生多模态理解。

#### （2）核心优势
- **超长上下文**：支持 $1\text{M}$ 至 $2\text{M}$ tokens 的上下文窗口，可一次性处理数小时视频、数万行代码或长篇文档。
- **视频长程推理**：能够直接在数小时的视频流中精准定位并提取细微信息（Needle In A Video Haystack）。

### 4. Llama 3.2 Vision
#### （1）核心定位
Meta 于 2024 年 9 月发布的首个开源多模态模型（包含 11B 和 90B 版本），旨在为开源社区提供可与闭源模型竞争的视觉能力。

#### （2）模型架构
- **Adapter 机制**：基于冻结的 Llama 3.1 文本模型，通过训练图像编码器和适配器（Adapter）层来引入视觉能力。
- **分块处理**：支持高分辨率图像处理，将图像切分为 Patch 序列后输入 Transformer。

### 5. Qwen2.5-VL
#### （1）核心定位
阿里巴巴于 2025 年 1 月发布的最新一代全能视觉语言模型。在 Qwen2-VL 的基础上，进一步强化了对空间和时间的感知能力，在长视频理解和智能 Agent 任务中表现尤为突出。

#### （2）核心改进
- **时空感知增强**：
  - **动态 FPS 采样**：针对不同长度的视频自动调整采样频率。
  - **绝对时间对齐 M-RoPE**：将多模态旋转位置编码（M-RoPE）扩展到时间维度，实现对小时级视频的秒级精准定位。
- **架构优化**：
  - **窗口注意力（Window Attention）**：在视觉编码器（ViT）中引入全注意力与窗口注意力的交替机制，大幅提升高分辨率输入的推理效率。
  - **结构统一**：ViT 架构与 LLM 保持高度一致（如采用 RMSNorm 和 SwiGLU 结构）。
- **原生动态分辨率**：支持从 $56 \times 56$ 到 $3584 \times 3584$ 像素的任意分辨率输入，无需缩放，保留最细微的视觉信息（如文档中的小字、图表细节）。

#### （3）典型应用
- **智能 Agent**：能够像人类一样操作手机、平板等设备，通过实时截屏理解 UI 并执行复杂指令。
- **超长视频分析**：支持处理 3 到 15 分钟甚至更长时间的视频，可用于监控分析、长视频摘要等场景。

### 6. Qwen2-VL
#### （1）核心定位
阿里巴巴 2024 年 8 月发布的视觉语言模型，引入了 **Naive Dynamic Resolution（原生动态分辨率）** 机制。

#### （2）关键技术
- **动态分辨率**：模型可以处理任意分辨率和比例的图像，将其动态映射为不同数量的视觉 Token。
- **M-RoPE（多模态旋转位置编码）**：有效地将 1D 文本位置信息与多维视觉空间位置信息进行融合。

### 7. BLIP-2

#### （2）模型结构
由预训练 Image Encoder（CLIP-ViT-L/14 或 EVA-CLIP-ViT-g/14）、预训练 LLM（Decoder-based 或 Encoder-Decoder-based）、Q-Former 组成；Q-Former 含 Image Transformer 和 Text Transformer，负责提取视觉特征并与文本交互。

#### （3）关键技巧（Tricks）
- 两阶段预训练：
  1. 表示学习阶段：Q-Former 连接冻结 Image Encoder，通过 ITC、ITM、ITG 目标训练，对齐图文特征。
  2. 生成预训练阶段：Q-Former 连接冻结 LLM，通过全连接层投影视觉特征至 LLM 文本嵌入维度，实现文本生成。
- 注意力掩码策略：不同预训练任务采用不同掩码（单模态自注意力掩码、双向自注意力掩码、多模态因果注意力掩码）控制交互方式。

#### （4）实验结论
更大的 Image Transformer 和 LLM 可提升 zero-shot VQA 任务性能；第一阶段表示学习对最终结果影响显著，消融后性能大幅下降。

### 8. LLaVA (Large Language and Vision Assistant)
#### （1）核心定位
端到端训练的大型多模态模型，连接 CLIP 视觉编码器与 Vicuna（LLaMA 结构）LLM，通过 GPT-4 生成的多模态指令遵循数据微调，可实现通用视觉语言理解。

#### （2）模型结构
视觉编码器采用 CLIP-ViT-L/14，提取图像 grid features，经投影层（Projection W）与文本特征维度对齐后，与文本 token 嵌入合并作为 LLM 输入。

#### （3）关键技巧（Tricks）
- GPT 辅助数据生成：基于 COCO 图像，生成对话、详细描述、复杂推理三类指令遵循数据（共 $158\text{K}$ 样本），采用 Captions 和 Boxes 两种符号表示提示 GPT-4。
- 指令微调：混合多模态和纯文本对话数据，增强模型指令跟随和对话能力。

#### （4）实验结论
全量数据训练效果最优，缺失某类数据会导致性能下降；与 GPT-4 集成后，在 Science QA 数据集上所有类别性能均提升，创下新 SOTA。

### 9. Qwen-VL (v1)
#### （1）核心定位
大型视觉-语言模型（LVLMs），基于 Qwen-LM 扩展视觉处理能力，可感知理解文本与图像，在 VQAv2、RefCOCO 等多个基准测试中表现优异。

#### （2）模型结构
- LLM：基于 Qwen-7B 预训练权重初始化，负责文本处理与生成。
- 视觉编码器：采用 ViT 架构，基于 Openclip ViT-bigG 预训练权重，将图像分割为 $14$ 像素步长的 Patch 生成特征序列。
- 位置感知视觉-语言适配器：通过单层交叉注意力机制压缩图像特征至固定长度 $256$，加入 $2\text{D}$ 绝对位置编码减少位置信息损失。

#### （3）训练流程
1. 预训练：使用 $14$ 亿清洗后的图文对，冻结 LLM，优化视觉编码器和适配器，输入图像分辨率 $224 \times 224$。
2. 多任务预训练：引入 $7$ 类视觉语言任务数据，解锁 LLM 训练整个模型，输入分辨率提升至 $448 \times 448$。
3. 监督微调：使用 $350\text{K}$ 多模态指令数据微调，冻结视觉编码器，优化 LLM 和适配器。

#### （4）实验结论
在多个视觉语言任务基准测试中达到 SOTA 水平，具备强大的图像理解、指令跟随和对话交互能力。

## 四、生成式多模态模型
### 1. Stable Diffusion (SD)
#### （1）核心定位
由 Stability AI、CompVis 和 Runway 联合开发的开源文生图（Text-to-Image）模型。它标志着生成式 AI 从像素级扩散向**潜空间扩散（Latent Diffusion）**的重大跨越，极大降低了计算资源需求，推动了开源社区的爆发。

#### （2）模型架构 (LDM)
Stable Diffusion 属于潜空间扩散模型（Latent Diffusion Model, LDM），其核心工作流程如下：
1.  **VAE (变分自编码器)**：
    - **Encoder**：将高维图像从像素空间（Pixel Space）压缩到低维潜空间（Latent Space），压缩倍率通常为 $8$（如 $512 \times 512 \times 3 \to 64 \times 64 \times 4$）。
    - **Decoder**：将去噪后的潜变量还原回像素空间，生成最终图像。
2.  **Denoising Network (去噪网络)**：
    - **U-Net / Transformer**：在潜空间中预测并去除噪声。SD 1.x/2.x/XL 使用增强型 U-Net，SD 3 转向 MM-DiT（多模态扩散 Transformer）。
3.  **Conditioning (条件注入)**：
    - **CLIP Text Encoder**：将文本 Prompt 转换为语义嵌入（Text Embeddings）。
    - **Cross-Attention (交叉注意力)**：将文本语义注入去噪网络，引导图像生成与文本对齐。

#### （3）版本演进
- **SD 1.5**：经典版本，使用单个 CLIP ViT-L/14 编码器，原生支持 $512 \times 512$ 分辨率。
- **SDXL**：引入双文本编码器（CLIP ViT-L + OpenCLIP ViT-bigG），采用 Base + Refiner 双阶段架构，支持原生 $1024 \times 1024$ 分辨率。
- **SD 3**：架构彻底重构，引入 **MM-DiT** 和 **流匹配（Flow Matching）** 技术，增加 T5-XXL 编码器，极大提升了多主题控制、文字渲染和语义遵循能力。

#### （4）核心公式
扩散模型包含前向加噪和反向去噪两个过程。在潜空间 $z$ 中，反向去噪的优化目标为：
$$ L_{LDM} = \mathbb{E}_{\mathcal{E}(x), \epsilon \sim \mathcal{N}(0,1), t} \left[ \| \epsilon - \epsilon_\theta(z_t, t, \tau_\theta(y)) \|_2^2 \right] $$
其中 $\mathcal{E}(x)$ 为图像的潜表示，$y$ 为文本条件，$\tau_\theta$ 为文本编码器，$\epsilon_\theta$ 为去噪网络预测的噪声。