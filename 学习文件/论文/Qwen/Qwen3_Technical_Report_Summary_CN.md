# Qwen3 技术报告总结

Qwen3 是阿里巴巴 Qwen 团队推出的最新一代大语言模型系列，其核心突破在于将“思考（Thinking）”与“非思考（Non-thinking）”模式统一集成在单个模型中，并实现了从 0.6B 到 235B 的全规模覆盖。

## 1. 核心亮点与创新
- **统一模式切换 (Unified Mode Switching)**：通过单一模型支持“思考模式”和“非思考模式”，用户可以通过 `/think` 和 `/no think` 标志动态控制。
- **思考预算控制 (Thinking Budget)**：引入了思考令牌预算机制，模型能够根据设定的预算长度，在思考中断时基于已有推理给出最终答案。
- **全系列卓越性能**：旗舰模型 Qwen3-235B-A22B 在推理、编程和数学任务上达到了 SOTA 水平，甚至在多项指标上超越了 OpenAI-o1 和 DeepSeek-R1。
- **强对弱蒸馏 (Strong-to-Weak Distillation)**：通过大模型蒸馏，使轻量化模型（如 0.6B、1.7B 等）也具备了强大的逻辑推理和模式切换能力。

## 2. 模型架构
Qwen3 延续了 Transformer 架构，并针对不同场景提供了密集（Dense）和专家混合（MoE）两种配置。

| 模型 | 类型 | 总参数量 | 激活参数量 | 层数 | 注意力机制 |
| :--- | :--- | :---: | :---: | :---: | :--- |
| **Qwen3-0.6B** | Dense | 0.6B | 0.6B | 16 | GQA |
| **Qwen3-1.7B** | Dense | 1.7B | 1.7B | 24 | GQA |
| **Qwen3-4B** | Dense | 4B | 4B | 36 | GQA |
| **Qwen3-8B** | Dense | 8B | 8B | 40 | GQA |
| **Qwen3-14B** | Dense | 14B | 14B | 48 | GQA |
| **Qwen3-32B** | Dense | 32B | 32B | 64 | GQA |
| **Qwen3-30B-A3B** | MoE | 30B | 3B | 48 | GQA (128 专家/激活 8) |
| **Qwen3-235B-A22B**| MoE | 235B | 22B | 94 | GQA (128 专家/激活 8) |

- **词表大小**：151,643 (BBPE)
- **上下文长度**：支持 128K 令牌。

## 3. 预训练 (Pre-training)
- **数据规模**：在 36 万亿 (36T) 令牌的大规模数据集上进行预训练。
- **多语言支持**：支持 119 种语言。
- **三阶段策略**：
  1. **通用预训练**：在大规模混合数据上构建基础能力。
  2. **领域增强**：针对代码、数学和特定知识领域进行针对性强化。
  3. **高质量微调**：在精选的高质量数据上进行最后阶段的性能优化。

## 4. 后训练 (Post-training)
Qwen3 的后训练流程分为四个关键阶段：
1. **长 CoT 冷启动**：通过精心筛选的高质量推理数据（数学、代码、STEM）训练初始的推理模式。
2. **推理强化学习 (Reasoning RL)**：采用 GRPO 算法，通过大规模批次训练，显著提升模型的逻辑推理深度和准确率。
3. **思考模式融合 (Thinking Mode Fusion)**：通过持续 SFT，将非思考任务数据与思考数据融合，并设计 Chat Template（`/think` 标志）实现模式切换。
4. **通用强化学习 (General RL)**：在指令遵循、格式对齐、偏好对齐、Agent 能力等 20 多个任务上进行综合优化，确保模型在各种场景下的稳定性。

## 5. 评估结果 (Evaluation)
- **旗舰模型 (235B)**：在思考模式下，数学 (AIME'25)、代码 (CodeForces Rating 2056) 等指标超越了 DeepSeek-R1，并在 Agent (BFCL v3) 任务上表现优异。
- **32B 密集模型**：在 32B 级别表现出统治力，性能可比肩甚至超越早期的 72B 级别模型（如 Qwen2.5-72B-Instruct）。
- **轻量化模型**：受益于强对弱蒸馏，Qwen3-14B 和 30B-A3B 在推理任务上显著优于同尺寸的竞争对手（如 Gemma-3-27B, Phi-4）。

## 6. 结论
Qwen3 通过统一思考模式和高效的蒸馏技术，成功地将复杂的推理能力下放到各种规模的模型中，为开发者提供了兼顾性能、成本和灵活性的 AI 基础设施。