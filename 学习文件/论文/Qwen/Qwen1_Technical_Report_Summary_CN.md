# Qwen1 技术报告详细总结

本文档对阿里巴巴发布的 Qwen（通义千问）第一代大型语言模型的技术报告进行了深度细化总结，涵盖预训练、对齐、专业模型及评估等各个小节。

## 1. 简介 (Introduction)
Qwen 是一个全面的语言模型系列，旨在提供高性能、可控且易于访问的模型。
- **模型规模**：发布了 1.8B、7B 和 14B 三种参数规模。
- **版本分类**：包含基座预训练模型（QWEN）和经过人类偏好对齐的对话模型（QWEN-CHAT）。
- **专业领域**：衍生出了 CODE-QWEN（代码）和 MATH-QWEN（数学）等专用模型，以及多模态模型 QWEN-VL。

## 2. 预训练 (Pretraining)

### 2.1 数据 (Data)
- **规模与多样性**：训练数据高达 3 万亿 (3T) tokens，涵盖网页、百科、书籍和代码。
- **预处理流程**：
    - **去重**：使用 MinHash 和 LSH 算法进行模糊去重。
    - **质量过滤**：结合规则和机器学习模型（语言模型、质量评分模型）剔除低质及攻击性内容。
    - **指令增强**：在预训练中加入高质量指令数据以增强零样本/少样本能力，并严格过滤 13-gram 重叠以防泄题。
- **语言支持**：重点支持中英双语，同时具备多语言能力。

### 2.2 分词 (Tokenization)
- **方法**：基于 BPE (Byte Pair Encoding)，使用 `tiktoken` 词表 `cl100k_base` 作为起点。
- **优化**：扩充了常用中文及多语言词汇，将数字拆分为单个数字。
- **效率**：词表大小约 152K，在中文、英文及代码上的压缩率极高，显著降低了推理成本。

### 2.3 架构 (Architecture)
- **基础**：基于改进的 Transformer 架构（类似 LLaMA）。
- **关键改进**：
    - **Embedding**：采用 Untied Embedding（不共享输入输出权重）以换取更高性能。
    - **位置编码**：使用 RoPE (Rotary Positional Embedding)，且在逆频率矩阵中使用 FP32 精度以确保准确性。
    - **Bias**：移除大部分层的偏置，但在 QKV 层保留偏置以提升模型的外推能力。
    - **归一化与激活**：使用 **RMSNorm** 提升稳定性与效率；采用 **SwiGLU** 激活函数（FFN 维度调整为隐藏层的 8/3）。

### 2.4 训练 (Training)
- **优化器**：AdamW ($\beta_1=0.9, \beta_2=0.95, \epsilon=10^{-8}$)。
- **策略**：余弦学习率衰减（最低衰减至峰值的 10%）；BFloat16 混合精度训练。
- **效率**：集成 **Flash Attention** 优化注意力计算。

### 2.5 上下文长度扩展 (Context Length Extension)
- **推理技术**：采用 **NTK-aware 插值**及**动态 NTK 插值**，无需重新训练即可扩展上下文。
- **注意力优化**：引入 **LogN-Scaling** 稳定注意力熵；采用**层级窗口注意力 (Layer-wise Window Attention)**，底层窗口短，高层窗口长。

## 3. 对齐 (Alignment)

### 3.1 监督微调 (SFT)
- **格式**：采用 **ChatML** 格式，清晰区分系统角色、用户和助手。
- **数据**：涵盖多种对话风格、任务指令及安全性数据（防范暴力、偏见等）。
- **训练细节**：AdamW 优化器，序列长度 2048，学习率峰值 $2 \times 10^{-6}$，进行 4000 步训练。

### 3.2 强化学习 (RLHF)
- **奖励模型 (RM)**：
    - **预训练 (PMP)**：在大量比较数据上进行偏好模型预训练。
    - **微调**：基于 6600 个标签的分类系统平衡采样，由人类标注模型响应。
    - **架构**：在 Qwen 基座上增加 Pooling 层提取奖励值。
- **PPO 训练**：包含策略、价值、参考和奖励四个模型。
- **对齐税缓解**：引入**预训练梯度 (Pretrained Gradient)** 以在对齐人类偏好的同时保留通用能力。

### 3.4 工具使用、代码解释器与 Agent
- **ReAct 提示**：模型能够根据提示选择工具、生成参数并处理外部观察。
- **代码解释器**：支持生成 Python 代码解决数学、数据可视化及文件处理任务。
- **Agent 框架**：适配 Hugging Face Transformers Agent，支持多模态工具调用。
- **数据生成**：使用 **Self-Instruct** 策略，通过 Qwen 自身生成高质量指令样本。

## 4. CODE-QWEN: 代码专用模型
- **预训练**：在 900 亿 (90B) 代码 tokens 上进行持续预训练，上下文扩展至 8192。
- **策略**：多阶段 SFT 策略效果最佳。
- **表现**：在 HumanEval、MBPP 及 HumanEvalPack 上表现强劲，CODE-QWEN-14B-CHAT 在代码生成上甚至能与 Starcoder 竞争。

## 5. MATH-QWEN: 数学专用模型
- **数据**：在增强的数学推理指令数据集上进行 SFT。
- **优化**：对用户输入进行 Mask 处理以加速收敛。
- **表现**：在 GSM8K 和 MATH 测试中远超同规模模型，MATH-QWEN-14B-CHAT 表现接近 GPT-3.5 和 Minerva-62B。

## 6. 结论 (Conclusion)
Qwen 证明了通过精细的数据处理、架构优化和对齐技术，中等规模的开源模型可以具备挑战更大规模模型乃至闭源模型的潜力。未来的方向包括进一步扩展规模、增强多模态能力及持续优化 Agent 表现。
